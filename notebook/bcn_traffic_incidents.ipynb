{
 "cells": [
  {
   "cell_type": "raw",
   "id": "3180c059-c8a2-40bf-82b5-d0f865c1f688",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79dd09ab-299e-43fe-aa40-f112a8a44052",
   "metadata": {},
   "source": [
    "## initialize Spark\n",
    "\n",
    "We will use local mode, where all the processing is done on a single machine. In case you need to install Apache Spark, there are two options, either go to the [Spark download page](https://spark.apache.org/downloads.html) and choose \"Pre-built for Apache Hadoop 3.3 and later\", or, install only PySpark Python library and its dependencies running `pip install pyspark[sql, ml, mllib]`.\n",
    "\n",
    "You will also need to have Java 8 or later installed in your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f479f7-c47d-4734-b15a-da7ca7aa4692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of findspark is to make it easier to find and use Spark from Python,\n",
    "# especially if you have not set the SPARK_HOME environment variable or\n",
    "# if your Spark and PySpark setup is not in your system's PATH.\n",
    "# If pyspark was NOT installed with pip, uncomment the next two lines\n",
    "#import findspark\n",
    "#findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "# initialize allocating 4 cores to Spark, my machine has 8\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"bcn_traffic_incidents\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fa3481-e24d-40f8-bad2-dda74bb95ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ed7b4-14b7-4f32-8e0a-c53bb5ce5483",
   "metadata": {},
   "source": [
    "## load datasets as DataFrame\n",
    "\n",
    "- df_incidents\n",
    "- df_incidents_type\n",
    "- df_incidents_vehicle\n",
    "- df_incidents_person\n",
    "- df_incidents_cause\n",
    "- df_incidents_driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada4f7e4-d9e4-40e7-ac53-fa54e80db101",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidents_file_path = \"../data/2023_accidents_gu_bcn.csv\"\n",
    "incidents_type_file_path = \"../data/2023_accidents_tipus_gu_bcn.csv\"\n",
    "incidents_vehicle_file_path = \"../data/2023_accidents_vehicles_gu_bcn.csv\"\n",
    "incidents_person_file_path = \"../data/2023_accidents_persones_gu_bcn.csv\"\n",
    "incidents_cause_file_path = \"../data/2023_accidents_causes_gu_bcn.csv\"\n",
    "incidentsdriver_file_path = \"../data/2023_accidents_causa_conductor_gu_bcn.csv\"\n",
    "\n",
    "df_incidents = spark.read.csv(incidents_file_path, header=True, inferSchema=True)\n",
    "df_incidents_type = spark.read.csv(incidents_type_file_path, header=True, inferSchema=True)\n",
    "df_incidents_vehicle = spark.read.csv(incidents_vehicle_file_path, header=True, inferSchema=True)\n",
    "df_incidents_person = spark.read.csv(incidents_person_file_path, header=True, inferSchema=True)\n",
    "df_incidents_cause = spark.read.csv(incidents_cause_file_path, header=True, inferSchema=True)\n",
    "df_incidents_driver = spark.read.csv(incidentsdriver_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0179674f-0292-462c-9fa8-ccd05b56ef62",
   "metadata": {},
   "source": [
    "## Data exploration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a17455ed-5e10-475c-9c1d-8bf8da99774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Numero_expedient: string (nullable = true)\n",
      " |-- Codi_districte: integer (nullable = true)\n",
      " |-- Nom_districte: string (nullable = true)\n",
      " |-- Codi_barri: integer (nullable = true)\n",
      " |-- Nom_barri: string (nullable = true)\n",
      " |-- Codi_carrer: integer (nullable = true)\n",
      " |-- Nom_carrer: string (nullable = true)\n",
      " |-- Num_postal : string (nullable = true)\n",
      " |-- Descripcio_dia_setmana: string (nullable = true)\n",
      " |-- NK_Any: integer (nullable = true)\n",
      " |-- Mes_any: integer (nullable = true)\n",
      " |-- Nom_mes: string (nullable = true)\n",
      " |-- Dia_mes: integer (nullable = true)\n",
      " |-- Hora_dia: integer (nullable = true)\n",
      " |-- Descripcio_torn: string (nullable = true)\n",
      " |-- Descripcio_causa_vianant: string (nullable = true)\n",
      " |-- Numero_morts: integer (nullable = true)\n",
      " |-- Numero_lesionats_lleus: integer (nullable = true)\n",
      " |-- Numero_lesionats_greus: integer (nullable = true)\n",
      " |-- Numero_victimes: integer (nullable = true)\n",
      " |-- Numero_vehicles_implicats: integer (nullable = true)\n",
      " |-- Coordenada_UTM_Y_ED50: string (nullable = true)\n",
      " |-- Coordenada_UTM_X_ED50: string (nullable = true)\n",
      " |-- Longitud_WGS84: double (nullable = true)\n",
      " |-- Latitud_WGS84: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_incidents.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f833107b-6369-48d8-af31-49d4346bc9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
